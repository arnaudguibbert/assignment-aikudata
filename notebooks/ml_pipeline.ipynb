{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline\n",
    "\n",
    "This notebook contain all the steps we performed during the ML exploration phase. Before running this notebook, make sure you read the [README.md](../README.md) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from assaiku.data import DataConfig\n",
    "\n",
    "from assaiku.model.configs import EvaluationConfig\n",
    "from assaiku.model.evaluation import evaluate_model\n",
    "from assaiku.model.processors import (\n",
    "    fit_processor,\n",
    "    initialize_feat_processor,\n",
    "    split_transform,\n",
    ")\n",
    "from assaiku.model.train import initialize_model, train_model\n",
    "from assaiku.model.configs import (\n",
    "    LogisticRegressionConfig,\n",
    "    EvaluationConfig,\n",
    "    LinearSVMConfig,\n",
    "    XGBConfig,\n",
    ")\n",
    "from assaiku.model import MLPipe\n",
    "from assaiku.data import DataPipe\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating clean data\n",
    "\n",
    "Let's generate clean data using the data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = DataConfig(perform_exploration=False)\n",
    "data_pipeline = DataPipe(data_config=data_config)\n",
    "data_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data generated by Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_parquet(path=data_config.train_data_out)\n",
    "test_data = pd.read_parquet(path=data_config.test_data_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the model\n",
    "\n",
    "We created pydantic basemodel, to configure the model and the preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = XGBConfig(\n",
    "    n_estimators=100, max_depth=7, learning_rate=1e-1, dimension_red=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiliaze the data processor and fit it\n",
    "\n",
    "First we will initialize our preprocessor, that will perform the following steps:\n",
    "- Standardize continuous features\n",
    "- One hot encode cetagorical features\n",
    "- `Optional` Perform feature augmentation with `RBF kernel`\n",
    "- `Optional` Perform dimensionality reduction with `PCA`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_processor, label_binarizer = initialize_feat_processor(\n",
    "    data_config=data_config, model_config=model_config\n",
    ")\n",
    "fit_processor(\n",
    "    train_data=train_data,\n",
    "    feature_cols=data_config.features,\n",
    "    pipeline=feat_processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and transform data for each data set\n",
    "\n",
    "Let's transform the data for the training set and testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, w_train = split_transform(\n",
    "    train_data,\n",
    "    feat_processor,\n",
    "    label_binarizer,\n",
    "    data_config=data_config,\n",
    ")\n",
    "x_test, y_test, w_test = split_transform(\n",
    "    test_data,\n",
    "    feat_processor,\n",
    "    label_binarizer,\n",
    "    data_config=data_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "Let's initiliaze the model and train it accordingly to what is specified in the configuration. In the training part, we put an option to rebalance the weights based on the label frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model(model_config=model_config)\n",
    "\n",
    "train_model(\n",
    "    model_config=model_config,\n",
    "    x_train=x_train,\n",
    "    y_train=y_train,\n",
    "    weights=w_train,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model\n",
    "\n",
    "Let's evaluate the performances of the model we trained on both train and test set. To evaluate the performances of the model we use the following metrics:\n",
    "- Recall\n",
    "- Precision\n",
    "- F1\n",
    "\n",
    "The choice of such metrics was justified by the unbalanced aspect of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_perf_0, train_perf_1 = evaluate_model(\n",
    "    model=model,\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    weights=w_train,\n",
    "    data_set=\"train\",\n",
    "    model_name=model_config.name,\n",
    ")\n",
    "\n",
    "test_perf_0, test_perf_1 = evaluate_model(\n",
    "    model=model,\n",
    "    x=x_test,\n",
    "    y=y_test,\n",
    "    weights=w_test,\n",
    "    data_set=\"test\",\n",
    "    model_name=model_config.name,\n",
    ")\n",
    "\n",
    "data = pd.DataFrame.from_records(\n",
    "    [train_perf_0, test_perf_0, test_perf_1, train_perf_1]\n",
    ")\n",
    "print(data.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare different models in one line\n",
    "\n",
    "All the steps presented above are wrapped into the ML pipeline. In addition it does the following:\n",
    "- Retrain several times the model with same configuration but different initialization to get meaningful metrics about performances\n",
    "- Train several different models based on the configurations given as input.\n",
    "- Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your evaluation config here\n",
    "eval_config = EvaluationConfig(\n",
    "    n_repet=2,\n",
    "    model_configs=[\n",
    "        XGBConfig(\n",
    "            n_estimators=100,\n",
    "            max_depth=7,\n",
    "            learning_rate=1e-1,\n",
    "            dimension_red=None,\n",
    "        ),\n",
    "        XGBConfig(weight_neg_factor=1, weight_pos_factor=1, dimension_red=50),\n",
    "        LinearSVMConfig(rbf_gamma=5e-5, C=100),\n",
    "        LinearSVMConfig(),\n",
    "        LogisticRegressionConfig(),\n",
    "        LogisticRegressionConfig(dimension_red=50),\n",
    "    ],\n",
    ")\n",
    "\n",
    "ml_pipeline = MLPipe(data_config=data_config, evaluation_config=eval_config)\n",
    "\n",
    "ml_pipeline.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
